---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |-
    // Destination: prometheus (prometheus)
    otelcol.exporter.prometheus "prometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "grafana-integration-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "grafana-integration-cluster"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    declare "grafana_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      declare "grafana_integration_discovery" {
        argument "namespaces" {
          comment = "The namespaces to look for targets in (default: [] is all namespaces)"
          optional = true
        }
    
        argument "field_selectors" {
          comment = "The field selectors to use to find matching targets (default: [])"
          optional = true
        }
    
        argument "label_selectors" {
          comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=grafana\"])"
          optional = true
        }
    
        argument "port_name" {
          comment = "The of the port to scrape metrics from (default: grafana)"
          optional = true
        }
    
        // grafana service discovery for all of the pods
        discovery.kubernetes "grafana_pods" {
          role = "pod"
    
          selectors {
            role = "pod"
            field = string.join(coalesce(argument.field_selectors.value, []), ",")
            label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=grafana"]), ",")
          }
    
          namespaces {
            names = coalesce(argument.namespaces.value, [])
          }
        }
    
        // grafana relabelings (pre-scrape)
        discovery.relabel "grafana_pods" {
          targets = discovery.kubernetes.grafana_pods.targets
    
          // keep only the specified metrics port name, and pods that are Running and ready
          rule {
            source_labels = [
              "__meta_kubernetes_pod_container_port_name",
              "__meta_kubernetes_pod_phase",
              "__meta_kubernetes_pod_ready",
              "__meta_kubernetes_pod_container_init",
            ]
            separator = "@"
            regex = coalesce(argument.port_name.value, "grafana") + "@Running@true@false"
            action = "keep"
          }
    
          
        
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        
        // set the workload to the controller kind and name
        rule {
          action = "lowercase"
          source_labels = ["__meta_kubernetes_pod_controller_kind"]
          target_label  = "workload_type"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_controller_name"]
          target_label  = "workload"
        }
        
        // remove the hash from the ReplicaSet
        rule {
          source_labels = [
            "workload_type",
            "workload",
          ]
          separator = "/"
          regex = "replicaset/(.+)-.+$"
          target_label  = "workload"
        }
        
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
        
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
        
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
        }
    
        export "output" {
          value = discovery.relabel.grafana_pods.output
        }
      }
    
      declare "grafana_integration_scrape" {
        argument "targets" {
          comment = "Must be a list() of targets"
        }
    
        argument "forward_to" {
          comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
        }
    
        argument "job_label" {
          comment = "The job label to add for all Loki metrics (default: integrations/grafana)"
          optional = true
        }
    
        argument "keep_metrics" {
          comment = "A regular expression of metrics to keep (default: see below)"
          optional = true
        }
    
        argument "drop_metrics" {
          comment = "A regular expression of metrics to drop (default: see below)"
          optional = true
        }
    
        argument "scrape_interval" {
          comment = "How often to scrape metrics from the targets (default: 60s)"
          optional = true
        }
    
        argument "max_cache_size" {
          comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
          optional = true
        }
    
        argument "clustering" {
          comment = "Whether or not clustering should be enabled (default: false)"
          optional = true
        }
    
        prometheus.scrape "grafana" {
          job_name = coalesce(argument.job_label.value, "integrations/grafana")
          forward_to = [prometheus.relabel.grafana.receiver]
          targets = argument.targets.value
          scrape_interval = coalesce(argument.scrape_interval.value, "60s")
    
          clustering {
            enabled = coalesce(argument.clustering.value, false)
          }
        }
    
        // grafana metric relabelings (post-scrape)
        prometheus.relabel "grafana" {
          forward_to = argument.forward_to.value
          max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
          // drop metrics that match the drop_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
            action = "drop"
          }
        }
      }
      
      grafana_integration_discovery "grafana" {
        namespaces = ["o11y"]
        label_selectors = ["app.kubernetes.io/name=grafana"]
        port_name = "grafana"
      }
      
      grafana_integration_scrape  "grafana" {
        targets = grafana_integration_discovery.grafana.output
        job_label = "integrations/grafana"
        clustering = true
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
    }
    grafana_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    
    
    
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="2.0.6", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="integrations", sources="%!s(<nil>)", version="1.0.0"} 1
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/api/push"
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "grafana-integration-cluster",
        "k8s_cluster_name" = "grafana-integration-cluster",
      }
    }
    
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
      
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
      
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
      
        // set the job label from the k8s.grafana.com/logs.job annotation if it exists
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
      
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
      
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
      
        // explicitly set service_name. if not set, loki will automatically try to populate a default.
        // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.name]
        // - pod.label[app.kubernetes.io/name]
        // - k8s.pod.name
        // - k8s.container.name
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_name",
            "__meta_kubernetes_pod_container_name",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_name"
        }
      
        // set service_namespace
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_namespace"]
          target_label = "service_namespace"
        }
      
        // set deployment_environment and deployment_environment_name
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_deployment_environment_name"]
          target_label = "deployment_environment_name"
        }
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_deployment_environment"]
          target_label = "deployment_environment"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:o11y);(?:grafana)"
          target_label = "job"
          replacement = "integrations/grafana"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:o11y);(?:grafana)"
          target_label = "instance"
          replacement = "grafana"
        }
      }
      
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
      
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
      
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
      
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
      
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
      
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
      
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
      
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
      
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
      
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
        // Integration: Loki
        stage.match {
          selector = "{job=\"integrations/grafana\",instance=\"grafana\",namespace=~\"o11y\"}"
        
          // extract some of the fields from the log line
          stage.logfmt {
            mapping = {
              "timestamp" = "t",
              "level" = "",
              "logger" = "",
              "type" = "",
            }
          }
        
          // set the level as a label
          stage.labels {
            values = {
              level = "level",
            }
          }
          // reset the timestamp to the extracted value
          stage.timestamp {
            source = "timestamp"
            format = "RFC3339Nano"
          }
          // remove the timestamp from the log line
          stage.replace {
            expression = "( t=[^ ]+\\s+)"
            replace = ""
          }
        }
      
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["app_kubernetes_io_name","container","instance","job","level","namespace","pod","service_name","service_namespace","deployment_environment","deployment_environment_name","integration"]
        }
      
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.loki.receiver,
      ]
    }
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8smon-alloy-metrics
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: k8smon
        namespace: default
      interval: 1m
  values: 
    alloy:
      clustering:
        enabled: true
        name: alloy-metrics
      configMap:
        create: false
      nodeSelector:
        kubernetes.io/os: linux
      podAnnotations:
        k8s.grafana.com/logs.job: integrations/alloy
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - CHOWN
          - DAC_OVERRIDE
          - FOWNER
          - FSETID
          - KILL
          - SETGID
          - SETUID
          - SETPCAP
          - NET_BIND_SERVICE
          - NET_RAW
          - SYS_CHROOT
          - MKNOD
          - AUDIT_WRITE
          - SETFCAP
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
    collectorName: alloy-metrics
    controller:
      replicas: 1
      type: statefulset
    crds:
      create: false
    enabled: true
    extraConfig: ""
    liveDebugging:
      enabled: false
    logging:
      format: logfmt
      level: info
    remoteConfig:
      auth:
        password: ""
        passwordFrom: ""
        passwordKey: password
        type: none
        username: ""
        usernameFrom: ""
        usernameKey: username
      enabled: false
      extraAttributes: {}
      pollFrequency: 5m
      secret:
        create: true
        embed: false
        name: ""
        namespace: ""
      url: ""
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8smon-alloy-logs
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: k8smon
        namespace: default
      interval: 1m
  values: 
    alloy:
      configMap:
        create: false
      mounts:
        dockercontainers: true
        varlog: true
      nodeSelector:
        kubernetes.io/os: linux
      podAnnotations:
        k8s.grafana.com/logs.job: integrations/alloy
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - CHOWN
          - DAC_OVERRIDE
          - FOWNER
          - FSETID
          - KILL
          - SETGID
          - SETUID
          - SETPCAP
          - NET_BIND_SERVICE
          - NET_RAW
          - SYS_CHROOT
          - MKNOD
          - AUDIT_WRITE
          - SETFCAP
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
    collectorName: alloy-logs
    controller:
      type: daemonset
    crds:
      create: false
    enabled: true
    extraConfig: ""
    liveDebugging:
      enabled: false
    logging:
      format: logfmt
      level: info
    remoteConfig:
      auth:
        password: ""
        passwordFrom: ""
        passwordKey: password
        type: none
        username: ""
        usernameFrom: ""
        usernameKey: username
      enabled: false
      extraAttributes: {}
      pollFrequency: 5m
      secret:
        create: true
        embed: false
        name: ""
        namespace: ""
      url: ""
---
# Source: k8s-monitoring/templates/temp_helm_repo.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: k8smon
  namespace: default
spec:
  interval: 1m
  url: https://grafana.github.io/helm-charts
