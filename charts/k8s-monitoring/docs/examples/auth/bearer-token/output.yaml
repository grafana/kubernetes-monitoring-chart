---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "prometheus-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  bearerToken: "c2FtcGxlLWJlYXJlci10b2tlbg=="
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |-
    // Destination: prometheus (prometheus)
    otelcol.exporter.prometheus "prometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        bearer_token = remote.kubernetes.secret.prometheus.data["bearerToken"]
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "bearer-token-example-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "bearer-token-example-cluster"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    remote.kubernetes.secret "prometheus" {
      name      = "prometheus-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Feature: Prometheus Operator Objects
    declare "prometheus_operator_objects" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
      
      // Prometheus Operator PodMonitor objects
      prometheus.operator.podmonitors "pod_monitors" {
        clustering {
          enabled = true
        }
        scrape {
          default_scrape_interval = "60s"
        }
        forward_to = argument.metrics_destinations.value
      }
      
      // Prometheus Operator Probe objects
      prometheus.operator.probes "pod_monitors" {
        clustering {
          enabled = true
        }
        scrape {
          default_scrape_interval = "60s"
        }
        forward_to = argument.metrics_destinations.value
      }
      
      // Prometheus Operator ServiceMonitor objects
      prometheus.operator.servicemonitors "service_monitors" {
        clustering {
          enabled = true
        }
        scrape {
          default_scrape_interval = "60s"
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    prometheus_operator_objects "feature" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    
    
    
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="2.0.6", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="applicationObservability", protocols="jaegerthrifthttp", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="prometheusOperatorObjects", version="1.0.0"} 1
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/loki/api/v1/push"
        bearer_token = env("LOKI_BEARER_TOKEN")
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "bearer-token-example-cluster",
        "k8s_cluster_name" = "bearer-token-example-cluster",
      }
    }
    
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
      
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
      
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
      
        // set the job label from the k8s.grafana.com/logs.job annotation if it exists
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
      
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
      
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
      
        // explicitly set service_name. if not set, loki will automatically try to populate a default.
        // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.name]
        // - pod.label[app.kubernetes.io/name]
        // - k8s.pod.name
        // - k8s.container.name
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_name",
            "__meta_kubernetes_pod_container_name",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_name"
        }
      
        // set service_namespace
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_namespace"]
          target_label = "service_namespace"
        }
      
        // set deployment_environment and deployment_environment_name
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_deployment_environment_name"]
          target_label = "deployment_environment_name"
        }
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_deployment_environment"]
          target_label = "deployment_environment"
        }
      }
      
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
      
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
      
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
      
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
      
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
      
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
      
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
      
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
      
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
      
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
      
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["app_kubernetes_io_name","container","instance","job","level","namespace","pod","service_name","service_namespace","deployment_environment","deployment_environment_name","integration"]
        }
      
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.loki.receiver,
      ]
    }
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-receiver
  namespace: default
data:
  config.alloy: |-
    // Destination: prometheus (prometheus)
    otelcol.exporter.prometheus "prometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        bearer_token = remote.kubernetes.secret.prometheus.data["bearerToken"]
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "bearer-token-example-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "bearer-token-example-cluster"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    remote.kubernetes.secret "prometheus" {
      name      = "prometheus-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/loki/api/v1/push"
        bearer_token = env("LOKI_BEARER_TOKEN")
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "bearer-token-example-cluster",
        "k8s_cluster_name" = "bearer-token-example-cluster",
      }
    }
    // Destination: tempo (otlp)
    otelcol.auth.bearer "tempo" {
      token = remote.kubernetes.secret.tempo.data["tempoBearerToken"]
    }
    
    otelcol.processor.attributes "tempo" {
      action {
        key = "cluster"
        action = "upsert"
        value = "bearer-token-example-cluster"
      }
      action {
        key = "k8s.cluster.name"
        action = "upsert"
        value = "bearer-token-example-cluster"
      }
      output {
        metrics = [otelcol.processor.transform.tempo.input]
        logs = [otelcol.processor.transform.tempo.input]
        traces = [otelcol.processor.transform.tempo.input]
      }
    }
    
    otelcol.processor.transform "tempo" {
      error_mode = "ignore"
    
      output {
        metrics = [otelcol.processor.batch.tempo.input]
        logs = [otelcol.processor.batch.tempo.input]
        traces = [otelcol.processor.batch.tempo.input]
      }
    }
    
    otelcol.processor.batch "tempo" {
      timeout = "2s"
      send_batch_size = 8192
      send_batch_max_size = 0
    
      output {
        metrics = [otelcol.exporter.otlp.tempo.input]
        logs = [otelcol.exporter.otlp.tempo.input]
        traces = [otelcol.exporter.otlp.tempo.input]
      }
    }
    otelcol.exporter.otlp "tempo" {
      client {
        endpoint = "http://tempo.tempo.svc:4317"
        auth = otelcol.auth.bearer.tempo.handler
        headers = {
          "X-Scope-OrgID" = nonsensitive(remote.kubernetes.secret.tempo.data["tenantId"]),
        }
        tls {
          insecure = false
          insecure_skip_verify = false
          ca_pem = nonsensitive(remote.kubernetes.secret.tempo.data["ca"])
          cert_pem = nonsensitive(remote.kubernetes.secret.tempo.data["cert"])
          key_pem = remote.kubernetes.secret.tempo.data["key"]
        }
      }
    }
    
    remote.kubernetes.secret "tempo" {
      name      = "my-tempo-secret"
      namespace = "tempo"
    }
    
    // Feature: Application Observability
    declare "application_observability" {
      argument "metrics_destinations" {
        comment = "Must be a list of metrics destinations where collected metrics should be forwarded to"
      }
    
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      argument "traces_destinations" {
        comment = "Must be a list of trace destinations where collected trace should be forwarded to"
      }
    
      // Jaeger Receiver  
      otelcol.receiver.jaeger "receiver" {
        protocols {
          thrift_http {
            endpoint = "0.0.0.0:14268"
          }
        }
      
        debug_metrics {
          disable_high_cardinality_metrics = true
        }
        output {
          traces = [otelcol.processor.resourcedetection.default.input]
        }
      }
    
      // Resource Detection Processor  
      otelcol.processor.resourcedetection "default" {
        detectors = ["env", "system"]
        system {
          hostname_sources = ["os"]
        }
      
        output {
          metrics = [otelcol.processor.k8sattributes.default.input]
          logs = [otelcol.processor.k8sattributes.default.input]
          traces = [otelcol.processor.k8sattributes.default.input]
        }
      }
    
      // K8s Attributes Processor  
      otelcol.processor.k8sattributes "default" {
        extract {
          metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
        }
        pod_association {
          source {
            from = "connection"
          }
        }
      
        output {
          metrics = [otelcol.processor.transform.default.input]
          logs = [otelcol.processor.transform.default.input]
          traces = [otelcol.processor.transform.default.input, otelcol.connector.host_info.default.input]
        }
      }
    
      // Host Info Connector  
      otelcol.connector.host_info "default" {
        host_identifiers = [ "k8s.node.name" ]
      
        output {
          metrics = [otelcol.processor.batch.default.input]
        }
      }
    
      // Transform Processor  
      otelcol.processor.transform "default" {
        error_mode = "ignore"
        log_statements {
          context = "resource"
          statements = [
            "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
            "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
            "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          ]
        }
      
        output {
          metrics = [otelcol.processor.batch.default.input]
          logs = [otelcol.processor.batch.default.input]
          traces = [otelcol.processor.batch.default.input]
        }
      }
    
      // Batch Processor  
      otelcol.processor.batch "default" {
        send_batch_size = 16384
        send_batch_max_size = 0
        timeout = "2s"
      
        output {
          metrics = argument.metrics_destinations.value
          logs = argument.logs_destinations.value
          traces = argument.traces_destinations.value
        }
      }
    }
    application_observability "feature" {
      metrics_destinations = [
        otelcol.exporter.prometheus.prometheus.input,
      ]
      logs_destinations = [
        otelcol.exporter.loki.loki.input,
      ]
      traces_destinations = [
        otelcol.processor.attributes.tempo.input,
      ]
    }
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8smon-alloy-metrics
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: k8smon
        namespace: default
      interval: 1m
  values: 
    alloy:
      clustering:
        enabled: true
        name: alloy-metrics
      configMap:
        create: false
      nodeSelector:
        kubernetes.io/os: linux
      podAnnotations:
        k8s.grafana.com/logs.job: integrations/alloy
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - CHOWN
          - DAC_OVERRIDE
          - FOWNER
          - FSETID
          - KILL
          - SETGID
          - SETUID
          - SETPCAP
          - NET_BIND_SERVICE
          - NET_RAW
          - SYS_CHROOT
          - MKNOD
          - AUDIT_WRITE
          - SETFCAP
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
    collectorName: alloy-metrics
    controller:
      replicas: 1
      type: statefulset
    crds:
      create: false
    enabled: true
    extraConfig: ""
    liveDebugging:
      enabled: false
    logging:
      format: logfmt
      level: info
    remoteConfig:
      auth:
        password: ""
        passwordFrom: ""
        passwordKey: password
        type: none
        username: ""
        usernameFrom: ""
        usernameKey: username
      enabled: false
      extraAttributes: {}
      pollFrequency: 5m
      secret:
        create: true
        embed: false
        name: ""
        namespace: ""
      url: ""
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8smon-alloy-logs
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: k8smon
        namespace: default
      interval: 1m
  values: 
    alloy:
      configMap:
        create: false
      extraEnv:
      - name: LOKI_BEARER_TOKEN
        value: sample-bearer-token
      mounts:
        dockercontainers: true
        varlog: true
      nodeSelector:
        kubernetes.io/os: linux
      podAnnotations:
        k8s.grafana.com/logs.job: integrations/alloy
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - CHOWN
          - DAC_OVERRIDE
          - FOWNER
          - FSETID
          - KILL
          - SETGID
          - SETUID
          - SETPCAP
          - NET_BIND_SERVICE
          - NET_RAW
          - SYS_CHROOT
          - MKNOD
          - AUDIT_WRITE
          - SETFCAP
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
    collectorName: alloy-logs
    controller:
      type: daemonset
    crds:
      create: false
    enabled: true
    extraConfig: ""
    liveDebugging:
      enabled: false
    logging:
      format: logfmt
      level: info
    remoteConfig:
      auth:
        password: ""
        passwordFrom: ""
        passwordKey: password
        type: none
        username: ""
        usernameFrom: ""
        usernameKey: username
      enabled: false
      extraAttributes: {}
      pollFrequency: 5m
      secret:
        create: true
        embed: false
        name: ""
        namespace: ""
      url: ""
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8smon-alloy-receiver
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: k8smon
        namespace: default
      interval: 1m
  values: 
    alloy:
      configMap:
        create: false
      extraPorts:
      - name: jaeger-grpc
        port: 14268
        protocol: TCP
        targetPort: 14268
      nodeSelector:
        kubernetes.io/os: linux
      podAnnotations:
        k8s.grafana.com/logs.job: integrations/alloy
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - CHOWN
          - DAC_OVERRIDE
          - FOWNER
          - FSETID
          - KILL
          - SETGID
          - SETUID
          - SETPCAP
          - NET_BIND_SERVICE
          - NET_RAW
          - SYS_CHROOT
          - MKNOD
          - AUDIT_WRITE
          - SETFCAP
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
    collectorName: alloy-receiver
    controller:
      type: daemonset
    crds:
      create: false
    enabled: true
    extraConfig: ""
    liveDebugging:
      enabled: false
    logging:
      format: logfmt
      level: info
    remoteConfig:
      auth:
        password: ""
        passwordFrom: ""
        passwordKey: password
        type: none
        username: ""
        usernameFrom: ""
        usernameKey: username
      enabled: false
      extraAttributes: {}
      pollFrequency: 5m
      secret:
        create: true
        embed: false
        name: ""
        namespace: ""
      url: ""
---
# Source: k8s-monitoring/templates/temp_helm_repo.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: k8smon
  namespace: default
spec:
  interval: 1m
  url: https://grafana.github.io/helm-charts
